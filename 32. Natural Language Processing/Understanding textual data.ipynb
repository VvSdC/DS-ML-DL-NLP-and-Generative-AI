{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffbc453f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1041c2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\SRIDATTA\n",
      "[nltk_data]     CHARAN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\SRIDATTA\n",
      "[nltk_data]     CHARAN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\SRIDATTA CHARAN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\SRIDATTA\n",
      "[nltk_data]     CHARAN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\SRIDATTA\n",
      "[nltk_data]     CHARAN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b36268",
   "metadata": {},
   "source": [
    "### Hierarchy of text\n",
    "\n",
    "Text is a collection of meaningful sentences. Each sentence comprises of many words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25f04fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''Natural Language Processing (NLP) enables machines to understand human language. \n",
    "For example, sentiment analysis can classify a movie review as positive or negative. \n",
    "Text summarization condenses long articles into short highlights. \n",
    "Named Entity Recognition (NER) identifies people, places, and organizations in text. \n",
    "Topic modeling groups documents by themes, such as sports or politics. \n",
    "Safe prompts include requests like “Explain how rainbows form.” \n",
    "Tricky but safe prompts include “Describe the role of war in history books.” \n",
    "Unsafe prompts might ask for harmful instructions, which must be filtered out.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeed0f38",
   "metadata": {},
   "source": [
    "### Tokens\n",
    "\n",
    "A meaningful unit fo text is called a token. Words are usually considered as tokens in Natural Lnaguage Processing. The process of breaking a text based on the token is called tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73504944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'enables', 'machines', 'to', 'understand', 'human', 'language', '.', 'For', 'example', ',', 'sentiment', 'analysis', 'can', 'classify', 'a', 'movie', 'review', 'as', 'positive', 'or', 'negative', '.', 'Text', 'summarization', 'condenses', 'long', 'articles', 'into', 'short', 'highlights', '.', 'Named', 'Entity', 'Recognition', '(', 'NER', ')', 'identifies', 'people', ',', 'places', ',', 'and', 'organizations', 'in', 'text', '.', 'Topic', 'modeling', 'groups', 'documents', 'by', 'themes', ',', 'such', 'as', 'sports', 'or', 'politics', '.', 'Safe', 'prompts', 'include', 'requests', 'like', '“', 'Explain', 'how', 'rainbows', 'form.', '”', 'Tricky', 'but', 'safe', 'prompts', 'include', '“', 'Describe', 'the', 'role', 'of', 'war', 'in', 'history', 'books.', '”', 'Unsafe', 'prompts', 'might', 'ask', 'for', 'harmful', 'instructions', ',', 'which', 'must', 'be', 'filtered', 'out', '.']\n"
     ]
    }
   ],
   "source": [
    "print(nltk.word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feebcf20",
   "metadata": {},
   "source": [
    "### Vocabulary\n",
    "\n",
    "The vocabulary of a text is the set of all unique tokens used in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "777df257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(', ')', ',', '.', 'Describe', 'Entity', 'Explain', 'For', 'Language', 'NER', 'NLP', 'Named', 'Natural', 'Processing', 'Recognition', 'Safe', 'Text', 'Topic', 'Tricky', 'Unsafe', 'a', 'analysis', 'and', 'articles', 'as', 'ask', 'be', 'books.', 'but', 'by', 'can', 'classify', 'condenses', 'documents', 'enables', 'example', 'filtered', 'for', 'form.', 'groups', 'harmful', 'highlights', 'history', 'how', 'human', 'identifies', 'in', 'include', 'instructions', 'into', 'language', 'like', 'long', 'machines', 'might', 'modeling', 'movie', 'must', 'negative', 'of', 'or', 'organizations', 'out', 'people', 'places', 'politics', 'positive', 'prompts', 'rainbows', 'requests', 'review', 'role', 'safe', 'sentiment', 'short', 'sports', 'such', 'summarization', 'text', 'the', 'themes', 'to', 'understand', 'war', 'which', '“', '”']\n"
     ]
    }
   ],
   "source": [
    "tokens =  nltk.word_tokenize(text)\n",
    "vocabulary = sorted(set(tokens))\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62ef3d0",
   "metadata": {},
   "source": [
    "## Punctuation\n",
    "\n",
    "Punctuation refers to symbols used to separate sentences and their elements and to clarify meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4a9a8f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Describe', 'Entity', 'Explain', 'For', 'Language', 'NER', 'NLP', 'Named', 'Natural', 'Processing', 'Recognition', 'Safe', 'Text', 'Topic', 'Tricky', 'Unsafe', 'a', 'analysis', 'and', 'articles', 'as', 'ask', 'be', 'books.', 'but', 'by', 'can', 'classify', 'condenses', 'documents', 'enables', 'example', 'filtered', 'for', 'form.', 'groups', 'harmful', 'highlights', 'history', 'how', 'human', 'identifies', 'in', 'include', 'instructions', 'into', 'language', 'like', 'long', 'machines', 'might', 'modeling', 'movie', 'must', 'negative', 'of', 'or', 'organizations', 'out', 'people', 'places', 'politics', 'positive', 'prompts', 'rainbows', 'requests', 'review', 'role', 'safe', 'sentiment', 'short', 'sports', 'such', 'summarization', 'text', 'the', 'themes', 'to', 'understand', 'war', 'which', '“', '”']\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "\n",
    "vocabulary_without_punct = []\n",
    "\n",
    "for word in vocabulary:\n",
    "    if word not in punctuation:\n",
    "        vocabulary_without_punct.append(word)\n",
    "\n",
    "print(vocabulary_without_punct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2ee2b3",
   "metadata": {},
   "source": [
    "### Parts of speech tagging\n",
    "\n",
    "Parts of speech refers to the category for which a word is assigned based on its function. Like english have 8 parts of speech - noun, verb, adjective, adverb, pronoun, determiner, preposition, conjunction and interjection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b0da74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Describe', 'NNP'), ('Entity', 'NNP'), ('Explain', 'NNP'), ('For', 'IN'), ('Language', 'NNP'), ('NER', 'NNP'), ('NLP', 'NNP'), ('Named', 'NNP'), ('Natural', 'NNP'), ('Processing', 'NNP'), ('Recognition', 'NNP'), ('Safe', 'NNP'), ('Text', 'NNP'), ('Topic', 'NNP'), ('Tricky', 'NNP'), ('Unsafe', 'NNP'), ('a', 'DT'), ('analysis', 'NN'), ('and', 'CC'), ('articles', 'NNS'), ('as', 'IN'), ('ask', 'NN'), ('be', 'VB'), ('books.', 'VBN'), ('but', 'CC'), ('by', 'IN'), ('can', 'MD'), ('classify', 'VB'), ('condenses', 'NNS'), ('documents', 'NNS'), ('enables', 'VBZ'), ('example', 'NN'), ('filtered', 'VBD'), ('for', 'IN'), ('form.', 'NN'), ('groups', 'NNS'), ('harmful', 'JJ'), ('highlights', 'NNS'), ('history', 'NN'), ('how', 'WRB'), ('human', 'JJ'), ('identifies', 'NNS'), ('in', 'IN'), ('include', 'JJ'), ('instructions', 'NNS'), ('into', 'IN'), ('language', 'NN'), ('like', 'IN'), ('long', 'JJ'), ('machines', 'NNS'), ('might', 'MD'), ('modeling', 'VBG'), ('movie', 'NN'), ('must', 'MD'), ('negative', 'VB'), ('of', 'IN'), ('or', 'CC'), ('organizations', 'NNS'), ('out', 'RP'), ('people', 'NNS'), ('places', 'NNS'), ('politics', 'NNS'), ('positive', 'JJ'), ('prompts', 'NNS'), ('rainbows', 'VBZ'), ('requests', 'NNS'), ('review', 'VBP'), ('role', 'NN'), ('safe', 'JJ'), ('sentiment', 'NN'), ('short', 'JJ'), ('sports', 'NNS'), ('such', 'JJ'), ('summarization', 'NN'), ('text', 'IN'), ('the', 'DT'), ('themes', 'NNS'), ('to', 'TO'), ('understand', 'VB'), ('war', 'NN'), ('which', 'WDT'), ('“', 'VBZ'), ('”', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# Using Penn treebank POS tagger\n",
    "\n",
    "pos_tagged_list = pos_tag(vocabulary_without_punct)\n",
    "print(pos_tagged_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483534e5",
   "metadata": {},
   "source": [
    "### Root of a word: Stemming\n",
    "\n",
    "Stemming is a technique used to find the root form of a word. The root form of a word is devoid of any affixes (prefixes ofr suffixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17d97dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Describe stemmed to -> describ\n",
      "Entity stemmed to -> entiti\n",
      "Explain stemmed to -> explain\n",
      "For stemmed to -> for\n",
      "Language stemmed to -> languag\n",
      "NER stemmed to -> ner\n",
      "NLP stemmed to -> nlp\n",
      "Named stemmed to -> name\n",
      "Natural stemmed to -> natur\n",
      "Processing stemmed to -> process\n",
      "Recognition stemmed to -> recognit\n",
      "Safe stemmed to -> safe\n",
      "Text stemmed to -> text\n",
      "Topic stemmed to -> topic\n",
      "Tricky stemmed to -> tricki\n",
      "Unsafe stemmed to -> unsaf\n",
      "a stemmed to -> a\n",
      "analysis stemmed to -> analysi\n",
      "and stemmed to -> and\n",
      "articles stemmed to -> articl\n",
      "as stemmed to -> as\n",
      "ask stemmed to -> ask\n",
      "be stemmed to -> be\n",
      "books. stemmed to -> books.\n",
      "but stemmed to -> but\n",
      "by stemmed to -> by\n",
      "can stemmed to -> can\n",
      "classify stemmed to -> classifi\n",
      "condenses stemmed to -> condens\n",
      "documents stemmed to -> document\n",
      "enables stemmed to -> enabl\n",
      "example stemmed to -> exampl\n",
      "filtered stemmed to -> filter\n",
      "for stemmed to -> for\n",
      "form. stemmed to -> form.\n",
      "groups stemmed to -> group\n",
      "harmful stemmed to -> harm\n",
      "highlights stemmed to -> highlight\n",
      "history stemmed to -> histori\n",
      "how stemmed to -> how\n",
      "human stemmed to -> human\n",
      "identifies stemmed to -> identifi\n",
      "in stemmed to -> in\n",
      "include stemmed to -> includ\n",
      "instructions stemmed to -> instruct\n",
      "into stemmed to -> into\n",
      "language stemmed to -> languag\n",
      "like stemmed to -> like\n",
      "long stemmed to -> long\n",
      "machines stemmed to -> machin\n",
      "might stemmed to -> might\n",
      "modeling stemmed to -> model\n",
      "movie stemmed to -> movi\n",
      "must stemmed to -> must\n",
      "negative stemmed to -> negat\n",
      "of stemmed to -> of\n",
      "or stemmed to -> or\n",
      "organizations stemmed to -> organ\n",
      "out stemmed to -> out\n",
      "people stemmed to -> peopl\n",
      "places stemmed to -> place\n",
      "politics stemmed to -> polit\n",
      "positive stemmed to -> posit\n",
      "prompts stemmed to -> prompt\n",
      "rainbows stemmed to -> rainbow\n",
      "requests stemmed to -> request\n",
      "review stemmed to -> review\n",
      "role stemmed to -> role\n",
      "safe stemmed to -> safe\n",
      "sentiment stemmed to -> sentiment\n",
      "short stemmed to -> short\n",
      "sports stemmed to -> sport\n",
      "such stemmed to -> such\n",
      "summarization stemmed to -> summar\n",
      "text stemmed to -> text\n",
      "the stemmed to -> the\n",
      "themes stemmed to -> theme\n",
      "to stemmed to -> to\n",
      "understand stemmed to -> understand\n",
      "war stemmed to -> war\n",
      "which stemmed to -> which\n",
      "“ stemmed to -> “\n",
      "” stemmed to -> ”\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemObj = SnowballStemmer(\"english\")\n",
    "\n",
    "for word in vocabulary_without_punct:\n",
    "    print(f\"{word} stemmed to -> {stemObj.stem(word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659c6a0e",
   "metadata": {},
   "source": [
    "### Base of a word: Lemmatization\n",
    "\n",
    "Lemmatization removes inflection and reduces the word to its base form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cf8e170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Describe -> lemmatized to Describe\n",
      "Entity -> lemmatized to Entity\n",
      "Explain -> lemmatized to Explain\n",
      "For -> lemmatized to For\n",
      "Language -> lemmatized to Language\n",
      "NER -> lemmatized to NER\n",
      "NLP -> lemmatized to NLP\n",
      "Named -> lemmatized to Named\n",
      "Natural -> lemmatized to Natural\n",
      "Processing -> lemmatized to Processing\n",
      "Recognition -> lemmatized to Recognition\n",
      "Safe -> lemmatized to Safe\n",
      "Text -> lemmatized to Text\n",
      "Topic -> lemmatized to Topic\n",
      "Tricky -> lemmatized to Tricky\n",
      "Unsafe -> lemmatized to Unsafe\n",
      "a -> lemmatized to a\n",
      "analysis -> lemmatized to analysis\n",
      "and -> lemmatized to and\n",
      "articles -> lemmatized to article\n",
      "as -> lemmatized to as\n",
      "ask -> lemmatized to ask\n",
      "be -> lemmatized to be\n",
      "books. -> lemmatized to books.\n",
      "but -> lemmatized to but\n",
      "by -> lemmatized to by\n",
      "can -> lemmatized to can\n",
      "classify -> lemmatized to classify\n",
      "condenses -> lemmatized to condense\n",
      "documents -> lemmatized to document\n",
      "enables -> lemmatized to enable\n",
      "example -> lemmatized to example\n",
      "filtered -> lemmatized to filter\n",
      "for -> lemmatized to for\n",
      "form. -> lemmatized to form.\n",
      "groups -> lemmatized to group\n",
      "harmful -> lemmatized to harmful\n",
      "highlights -> lemmatized to highlight\n",
      "history -> lemmatized to history\n",
      "how -> lemmatized to how\n",
      "human -> lemmatized to human\n",
      "identifies -> lemmatized to identify\n",
      "in -> lemmatized to in\n",
      "include -> lemmatized to include\n",
      "instructions -> lemmatized to instructions\n",
      "into -> lemmatized to into\n",
      "language -> lemmatized to language\n",
      "like -> lemmatized to like\n",
      "long -> lemmatized to long\n",
      "machines -> lemmatized to machine\n",
      "might -> lemmatized to might\n",
      "modeling -> lemmatized to model\n",
      "movie -> lemmatized to movie\n",
      "must -> lemmatized to must\n",
      "negative -> lemmatized to negative\n",
      "of -> lemmatized to of\n",
      "or -> lemmatized to or\n",
      "organizations -> lemmatized to organizations\n",
      "out -> lemmatized to out\n",
      "people -> lemmatized to people\n",
      "places -> lemmatized to place\n",
      "politics -> lemmatized to politics\n",
      "positive -> lemmatized to positive\n",
      "prompts -> lemmatized to prompt\n",
      "rainbows -> lemmatized to rainbows\n",
      "requests -> lemmatized to request\n",
      "review -> lemmatized to review\n",
      "role -> lemmatized to role\n",
      "safe -> lemmatized to safe\n",
      "sentiment -> lemmatized to sentiment\n",
      "short -> lemmatized to short\n",
      "sports -> lemmatized to sport\n",
      "such -> lemmatized to such\n",
      "summarization -> lemmatized to summarization\n",
      "text -> lemmatized to text\n",
      "the -> lemmatized to the\n",
      "themes -> lemmatized to theme\n",
      "to -> lemmatized to to\n",
      "understand -> lemmatized to understand\n",
      "war -> lemmatized to war\n",
      "which -> lemmatized to which\n",
      "“ -> lemmatized to “\n",
      "” -> lemmatized to ”\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmaObj = WordNetLemmatizer()\n",
    "\n",
    "for word in vocabulary_without_punct:\n",
    "    print(f\"{word} -> lemmatized to {lemmaObj.lemmatize(word,pos='v')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800741ed",
   "metadata": {},
   "source": [
    "### Stop words\n",
    "\n",
    "Stop words are most commonly occuring words in the text like the this, that etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67a2a145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Describe', 'Entity', 'Explain', 'For', 'Language', 'NER', 'NLP', 'Named', 'Natural', 'Processing', 'Recognition', 'Safe', 'Text', 'Topic', 'Tricky', 'Unsafe', 'analysis', 'articles', 'ask', 'books.', 'classify', 'condenses', 'documents', 'enables', 'example', 'filtered', 'form.', 'groups', 'harmful', 'highlights', 'history', 'human', 'identifies', 'include', 'instructions', 'language', 'like', 'long', 'machines', 'might', 'modeling', 'movie', 'must', 'negative', 'organizations', 'people', 'places', 'politics', 'positive', 'prompts', 'rainbows', 'requests', 'review', 'role', 'safe', 'sentiment', 'short', 'sports', 'summarization', 'text', 'themes', 'understand', 'war', '“', '”']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_set = set(stopwords.words(\"english\"))\n",
    "\n",
    "without_stop_words = []\n",
    "for word in vocabulary_without_punct:\n",
    "    if word not in stopwords_set:\n",
    "        without_stop_words.append(word)\n",
    "\n",
    "print(without_stop_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
